{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f1d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laksh\\anaconda3\\envs\\ner_bert\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:__main__:Dataset loaded successfully\n",
      "INFO:__main__:Tokenizer loaded successfully\n",
      "INFO:__main__:Data augmentation completed\n",
      "INFO:__main__:Tokenization completed\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Model loaded successfully\n",
      "INFO:__main__:Training arguments set\n",
      "C:\\Users\\laksh\\AppData\\Local\\Temp\\ipykernel_39432\\1564437038.py:170: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "INFO:__main__:Trainer initialized\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='8780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  29/8780 06:08 < 33:12:34, 0.07 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForTokenClassification,\n",
    ")\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1. Load dataset and labels\n",
    "try:\n",
    "    dataset = load_dataset(\"conll2003\")\n",
    "    label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "    num_labels = len(label_names)\n",
    "    logger.info(\"Dataset loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# 2. Load tokenizer\n",
    "try:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    logger.info(\"Tokenizer loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Augmentation dictionary\n",
    "ENTITY_DICT = {\n",
    "    \"PER\": [\"Barack Obama\", \"Angela Merkel\", \"Elon Musk\", \"Narendra Modi\"],\n",
    "    \"LOC\": [\"Paris\", \"New York\", \"Delhi\", \"Tokyo\"],\n",
    "    \"ORG\": [\"Google\", \"UN\", \"Tesla\", \"Apple\"]\n",
    "}\n",
    "\n",
    "# 4. Entity swap augmentation\n",
    "def swap_named_entities(example):\n",
    "    tokens = example[\"tokens\"]\n",
    "    ner_tags = example[\"ner_tags\"]\n",
    "    new_tokens = []\n",
    "    new_tags = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        tag = ner_tags[i]\n",
    "        tag_name = label_names[tag]\n",
    "        if tag_name.startswith(\"B-\"):\n",
    "            entity_type = tag_name[2:]\n",
    "            entity_tokens = [tokens[i]]\n",
    "            i += 1\n",
    "            while i < len(tokens) and label_names[ner_tags[i]].startswith(\"I-\"):\n",
    "                entity_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "            if entity_type in ENTITY_DICT:\n",
    "                replacement = random.choice(ENTITY_DICT[entity_type]).split()\n",
    "                new_tokens.extend(replacement)\n",
    "                new_tags.extend([label_names.index(\"B-\" + entity_type)] +\n",
    "                                [label_names.index(\"I-\" + entity_type)] * (len(replacement) - 1))\n",
    "            else:\n",
    "                new_tokens.extend(entity_tokens)\n",
    "                new_tags.extend([label_names.index(\"B-\" + entity_type)] +\n",
    "                                [label_names.index(\"I-\" + entity_type)] * (len(entity_tokens) - 1))\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            new_tags.append(ner_tags[i])\n",
    "            i += 1\n",
    "    return {\"tokens\": new_tokens, \"ner_tags\": new_tags}\n",
    "\n",
    "# 5. Augment + combine\n",
    "try:\n",
    "    augmented = dataset[\"train\"].map(swap_named_entities)\n",
    "    augmented_train_dataset = concatenate_datasets([dataset[\"train\"], augmented])\n",
    "    logger.info(\"Data augmentation completed\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during augmentation: {e}\")\n",
    "    raise\n",
    "\n",
    "# 6. Tokenize and align\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, padding='max_length', is_split_into_words=True, max_length=128\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# 7. Tokenize datasets\n",
    "try:\n",
    "    tokenized_train = augmented_train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    tokenized_val = dataset[\"validation\"].map(tokenize_and_align_labels, batched=True)\n",
    "    logger.info(\"Tokenization completed\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during tokenization: {e}\")\n",
    "    raise\n",
    "\n",
    "# 8. Load model\n",
    "try:\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=num_labels\n",
    "    )\n",
    "    logger.info(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# 9. Evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    true_preds = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_preds),\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "    }\n",
    "\n",
    "# 10. Training arguments\n",
    "try:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./ner_model\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU is available\n",
    "    )\n",
    "    logger.info(\"Training arguments set\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error setting training arguments: {e}\")\n",
    "    raise\n",
    "\n",
    "# 11. Trainer\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    logger.info(\"Trainer initialized\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing trainer: {e}\")\n",
    "    raise\n",
    "\n",
    "# 12. Train\n",
    "try:\n",
    "    trainer.train()\n",
    "    logger.info(\"Training completed successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Save model\n",
    "trainer.save_model(\"./ner_model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfc218b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0 4.52.4 2.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import accelerate, transformers, torch, datasets, seqeval, numpy\n",
    "print(accelerate.__version__, transformers.__version__, torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33459a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laksh\\anaconda3\\envs\\ner_bert\\python.exe\n",
      "3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 12:58:53) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0b643c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "try:\n",
    "    training_args = TrainingArguments(output_dir=\"./test\", eval_strategy=\"epoch\")\n",
    "    print(\"TrainingArguments created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb25ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
